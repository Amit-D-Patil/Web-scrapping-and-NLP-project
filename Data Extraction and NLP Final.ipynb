{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[{"file_id":"10B2s5af8INkHgIz_7XBjW3iNDbGY7ltx","timestamp":1677073966708}],"collapsed_sections":["vncDsAP0Gaoa","FJNUwmbgGyua","yQaldy8SH6Dl","mDgbUHAGgjLW","O_i_v8NEhb9l","Y3lxredqlCYt","x71ZqKXriCWQ","7hBIi_osiCS2","JlHwYmJAmNHm","35m5QtbWiB9F","PoPl-ycgm1ru","gCX9965dhzqZ"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Project Name**    - **Data Extraction and NLP**\n","\n"],"metadata":{"id":"vncDsAP0Gaoa"}},{"cell_type":"markdown","source":["**Introduction:**\n","\n","The objective of this project is to extract textual data articles from the given URLs and perform text analysis to compute variables that are explained below. The project aims to develop a Python program that uses web scraping libraries such as BeautifulSoup or Scrapy to extract article text from each URL, and perform text analysis on each article to compute various variables. The output of the program is a CSV or Excel file containing the input variables and the computed variables.\n","\n"," **Input Data:**\n","\n","The input data is provided in the form of an Excel file named Input.xlsx, which contains a list of URLs along with their associated URL_ID, Title, and Publication Date. The program will use this file to extract article text for each URL, and perform text analysis on each article.\n","\n","**Output Data Structure:**\n","\n","The output data structure is provided in the form of an Excel file named Output Data Structure.xlsx. This file contains a list of input variables along with computed variables such as Positive Score, Negative Score, Polarity Score, Subjectivity Score, Average Sentence Length, Percentage of Complex Words, FOG Index, Average Number of Words per Sentence, Complex Word Count, Word Count, Syllable per Word, Personal Pronouns, and Average Word Length. The program will compute these variables for each article and store the results in a new CSV or Excel file with the same format as Output Data Structure.xlsx.\n","\n","**Background:**\n","\n","Web scraping is the process of extracting data from websites using automated tools such as web crawlers or scraping scripts. It involves parsing HTML or XML code to extract specific data elements, such as text or images. Web scraping can be useful for data mining, market research, or content aggregation.\n","\n","Text analysis is the process of analyzing unstructured text data to extract meaningful insights or patterns. It involves techniques such as natural language processing (NLP), sentiment analysis, and topic modeling. Text analysis can be useful for understanding customer feedback, social media sentiment, or content classification.\n","\n","In this project, web scraping and text analysis are combined to extract article text from URLs and compute various variables such as sentiment scores, readability measures, and word usage statistics. The program will use web scraping libraries such as BeautifulSoup or Scrapy to extract article text, and NLP libraries such as NLTK or spaCy to perform text analysis."],"metadata":{"id":"beRrZCGUAJYm"}},{"cell_type":"markdown","source":["# **Project Summary -**"],"metadata":{"id":"FJNUwmbgGyua"}},{"cell_type":"markdown","source":["\n","\n","The objective of this project is to extract article text from a list of URLs provided in an Excel file and perform text analysis on each article to compute various variables. The project involves developing a Python program that uses web scraping libraries such as BeautifulSoup or Scrapy to extract article text from each URL, and NLP libraries such as NLTK or spaCy to perform text analysis on each article.\n","\n","The input data is provided in the form of an Excel file named Input.xlsx, which contains a list of URLs along with their associated URL_ID, Title, and Publication Date. The program will use this file to extract article text for each URL, and perform text analysis on each article.\n","\n","The output data structure is provided in the form of an Excel file named Output Data Structure.xlsx, which contains a list of input variables along with computed variables such as Positive Score, Negative Score, Polarity Score, Subjectivity Score, Average Sentence Length, Percentage of Complex Words, FOG Index, Average Number of Words per Sentence, Complex Word Count, Word Count, Syllable per Word, Personal Pronouns, and Average Word Length. The program will compute these variables for each article and store the results in a new CSV or Excel file with the same format as Output Data Structure.xlsx.\n","\n","The timeline for this project is six days, and the project can be adjusted based on the complexity of the web scraping and text analysis tasks. Web scraping and text analysis are combined to extract article text from URLs and compute various variables such as sentiment scores, readability measures, and word usage statistics.\n","\n","Overall, this project aims to provide a useful tool for content aggregation, market research, and data analysis, which can be used in various domains such as journalism, social media, and e-commerce."],"metadata":{"id":"F6v_1wHtG2nS"}},{"cell_type":"markdown","source":["# **Problem Statement**\n"],"metadata":{"id":"yQaldy8SH6Dl"}},{"cell_type":"markdown","source":["In today's digital era, there is a vast amount of textual data available online. This data is a valuable source of information for various applications, such as market research, content aggregation, and data analysis. However, the challenge is to extract the relevant textual data and analyze it effectively to derive meaningful insights.\n","\n","The problem statement for this project is to extract article text from a list of URLs and perform text analysis to compute various variables. The challenge is to develop an efficient web scraping program that can extract article text accurately from each URL, excluding website headers, footers, and any other non-relevant information. The program must also perform text analysis effectively and compute variables such as sentiment scores, readability measures, and word usage statistics, which are essential for data analysis.\n","\n","The problem statement also involves handling various data-related challenges, such as missing data, inconsistent data formats, and data quality issues. The program must handle these challenges and provide accurate and reliable results.\n","\n","The overall objective of this project is to provide a useful tool for content aggregation, market research, and data analysis, which can be used in various domains such as journalism, social media, and e-commerce. By addressing the challenges of web scraping and text analysis, this project aims to facilitate the extraction and analysis of textual data, leading to meaningful insights and better decision-making."],"metadata":{"id":"DpeJGUA3kjGy"}},{"cell_type":"markdown","source":["# **General Guidelines** : -  "],"metadata":{"id":"mDgbUHAGgjLW"}},{"cell_type":"markdown","source":["General Guidelines for Text Analysis of Articles:\n","\n","**1.Data Extraction:**\n","\n","  a. Use the pandas library in Python to read the URLs from the Input.xlsx file.\n","\n","  b. Iterate over each URL and use libraries such as Beautifulsoup, Scrapy, or Selenium to extract the article text.\n","\n","c. Clean the extracted text by removing any unnecessary information such as website headers, footers, and advertisements.\n","\n","d. Save each extracted article in a text file with URL_ID as its file name.\n","\n","\n","\n","**2.Data Analysis:**\n","\n","a. Use the NLTK library in Python for the textual analysis of each extracted article.\n","\n","b. Compute the variables as per the definitions given in the Text Analysis.docx file.\n","\n","c. Save the computed variables in a dictionary or a list.\n","\n","d. Iterate over all the URLs in Input.xlsx and perform the above steps.\n","\n","\n","\n","**3.Output Data Structure:**\n","\n","a. Create a pandas dataframe with the columns as given in the Output Data Structure.xlsx file.\n","\n","b. Populate the dataframe with the extracted variables and the computed variables.\n","\n","c. Export the dataframe to a CSV or Excel file as per the format given in the Output Data Structure.xlsx file.\n","\n","**4.Code to Generate Expected Output:**\n","\n","a. Load the URLs from Input.xlsx using the pandas library.\n","\n","b. Iterate over each URL and use libraries such as Beautifulsoup, Scrapy, or \n","Selenium to extract the article text.\n","\n","c. Perform textual analysis on each extracted article text to compute the \n","required variables using the NLTK library.\n","\n","d. Save the computed variables in a dictionary or a list.\n","\n","e. Populate the pandas dataframe with the extracted variables and the computed variables.\n","\n","f. Export the dataframe to a CSV or Excel file as per the format given in the Output Data Structure.xlsx file."],"metadata":{"id":"ZrxVaUj-hHfC"}},{"cell_type":"markdown","source":["# ***Let's Begin !***"],"metadata":{"id":"O_i_v8NEhb9l"}},{"cell_type":"markdown","source":["### Import Libraries"],"metadata":{"id":"Y3lxredqlCYt"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"tW6hT-NLa3JF"},"outputs":[],"source":["import os\n","import string\n","import re\n","import pandas as pd\n","import requests\n","from bs4 import BeautifulSoup\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.corpus import cmudict\n","!pip install textstat\n","!pip install vaderSentiment\n","from textstat import flesch_reading_ease, smog_index, automated_readability_index\n","from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n","from textblob import TextBlob\n","\n","# Download required NLTK packages\n","import nltk\n","nltk.download('punkt')\n","nltk.download('cmudict')\n","\n","# Download required TextBlob corpora\n","from textblob import TextBlob"]},{"cell_type":"markdown","source":["### Dataset Loading"],"metadata":{"id":"3RnN4peoiCZX"}},{"cell_type":"code","source":["# Read the input file\n","dir_path='/content/drive/MyDrive/Blackcoffer Project/Copy of Input.xlsx'\n","input_df = pd.read_excel(dir_path)"],"metadata":{"id":"4ABKJ9GdbaBz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"5jRxHh04cHB9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Code to Create a folder to store the text files"],"metadata":{"id":"x71ZqKXriCWQ"}},{"cell_type":"code","source":["# Create a folder to store the text files\n","if not os.path.exists('/content/drive/MyDrive/Blackcoffer Project/text_files'):\n","    os.makedirs('/content/drive/MyDrive/Blackcoffer Project/text_files')\n","\n","url_list=[]\n","# Loop through each URL and extract the article text\n","for index, row in input_df.iterrows():\n","    url_id = row['URL_ID']\n","    url = row['URL']\n","    url_list.append(url)\n","    response = requests.get(url)\n","    soup = BeautifulSoup(response.content, 'html.parser')\n","\n","    # Find the article title and text using appropriate tags\n","    article_title = ''\n","    article_text = ''\n","    title_tag = soup.find('h1')\n","    if title_tag:\n","        article_title = title_tag.text.strip()\n","\n","    for p in soup.find_all('p'):\n","        if p.text.strip() != '':\n","            article_text += p.text.strip() + ' '\n","\n","    # Save the extracted text to a text file with URL_ID as its file name\n","    if article_title != '' and article_text != '':\n","        with open(f'/content/drive/MyDrive/Blackcoffer Project/textFiles1/{url_id}.txt', 'w', encoding='utf-8') as f:\n","            f.write(f'{article_title}\\n{article_text}')\n","\n","        print(f'Saved text file for {url_id}')\n","    else:\n","        print(f'Error extracting text for {url_id}')"],"metadata":{"id":"b3pK5py1bfCr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Loading data which is required for anlyze/checking"],"metadata":{"id":"7hBIi_osiCS2"}},{"cell_type":"code","source":["\n","\n","# Load data into pandas dataframe\n","df = pd.read_excel(\"/content/drive/MyDrive/Blackcoffer Project/Copy of Input.xlsx\")\n","\n","stop_words_folder='/content/drive/MyDrive/Blackcoffer Project/StopWords'\n","# Load the stop words\n","stop_words = set()\n","for filename in os.listdir(stop_words_folder):\n","    with open(os.path.join(stop_words_folder, filename), \"r\", encoding=\"latin-1\") as f:\n","\n","        words = f.read().splitlines()\n","        stop_words.update(words)"],"metadata":{"id":"YxclGCkeuE6g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Define a function to clean the text by removing stop words and punctuation"],"metadata":{"id":"JlHwYmJAmNHm"}},{"cell_type":"code","source":["\n","# Define a function to clean the text by removing stop words and punctuation\n","def clean_text(text):\n","    # Remove punctuation\n","    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n","    # Convert to lowercase\n","    text = text.lower()\n","    # Tokenize the text\n","    tokens = word_tokenize(text)\n","    # Remove stop words\n","    tokens = [token for token in tokens if token not in stop_words]\n","    # Join the tokens back into a string\n","    text = \" \".join(tokens)\n","    return text\n","\n","# load and analyze each article file\n","for filename in os.listdir('/content/drive/MyDrive/Blackcoffer Project/textFiles1'):\n","    if filename.endswith('.0.txt'):\n","        file_path = os.path.join('/content/drive/MyDrive/Blackcoffer Project/textFiles1', filename)\n","        with open(file_path, 'r') as f:\n","            article = f.read()\n","            cleaned_text = clean_text(article)\n"],"metadata":{"id":"FtnByA_tuSfW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Duplicate Values"],"metadata":{"id":"35m5QtbWiB9F"}},{"cell_type":"code","source":["# Dataset Duplicate Value Count"],"metadata":{"id":"1sLdpKYkmox0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Missing Values/Null Values"],"metadata":{"id":"PoPl-ycgm1ru"}},{"cell_type":"code","source":["# Missing Values/Null Values Count"],"metadata":{"id":"GgHWkxvamxVg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualizing the missing values"],"metadata":{"id":"3q5wnI3om9sJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Functions that will calculate requied variables and stored to file "],"metadata":{"id":"H0kj-8xxnORC"}},{"cell_type":"code","source":["#2 Final code\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","master_dictionary_folder='/content/drive/MyDrive/Blackcoffer Project/MasterDictionary'\n","\n","# Define a function to calculate the variables for a given text file\n","def calculate_variables(file_id, article):\n","    # Clean the text\n","    cleaned_text = article.lower().strip()\n","    cleaned_text = re.sub(r'[^\\w\\s]','',cleaned_text)\n","    # Load the master dictionary\n","    positive_words = set()\n","    negative_words = set()\n","    with open(os.path.join(master_dictionary_folder, \"positive-words.txt\"), \"r\",encoding='latin-1') as f:\n","        words = f.read().splitlines()\n","        positive_words.update(words)\n","    with open(os.path.join(master_dictionary_folder, \"negative-words.txt\"), \"r\",encoding='latin-1') as f:\n","        words = f.read().splitlines()\n","        negative_words.update(words)\n","    # Calculate the positive and negative scores\n","    positive_score = 0\n","    negative_score = 0\n","    for word in word_tokenize(cleaned_text):\n","        if word in positive_words:\n","            positive_score += 1\n","        elif word in negative_words:\n","            negative_score += 1\n","    # Calculate the polarity score\n","    polarity_score = (positive_score - negative_score) / (positive_score + negative_score + 0.000001)\n","    # Calculate the subjectivity score\n","    subjectivity_score = (positive_score + negative_score) / (len(cleaned_text.split()) + 0.000001)\n","    return {\n","        \"file_id\": file_id,\n","        \"positive_score\": positive_score,\n","        \"negative_score\": negative_score,\n","        \"polarity_score\": polarity_score,\n","        \"subjectivity_score\": subjectivity_score,\n","    }\n","\n","# define function to count number of syllables in a word\n","def syllable_count(word):\n","    vowels = \"aeiouy\"\n","    count = 0\n","    if word[0] in vowels:\n","        count += 1\n","    for i in range(1, len(word)):\n","        if word[i] in vowels and word[i-1] not in vowels:\n","            count += 1\n","    if word.endswith('e'):\n","        count -= 1\n","    if count == 0:\n","        count += 1\n","    return count\n","\n","# define function to calculate the FOG index\n","def fog_index(word_count, complex_word_count, sentence_count):\n","    return 0.4 * ((word_count / sentence_count) + (100 * complex_word_count / word_count))\n","\n","\n","# define function to perform text analysis tasks\n","def analyze_text(text):\n","    # tokenize text into sentences and words\n","    sentences = sent_tokenize(text)\n","    words = word_tokenize(text)\n","\n","    # calculate number of words and complex words\n","    word_count = len(words)\n","    complex_words = [word for word in words if syllable_count(word) >= 3]\n","    complex_word_count = len(complex_words)\n","\n","\n","# calculate average number of words per sentence and average sentence length\n","    avg_words_per_sentence = word_count / len(sentences)\n","    avg_sentence_length = sum(len(sentence.split()) for sentence in sentences) / len(sentences)\n","    avg_word_length = sum(len(word) for word in words) / word_count\n","\n","    # count personal pronouns and calculate polarity and subjectivity scores\n","    personal_pronouns = len(re.findall(r'\\b(I|me|my|mine|we|us|our|ours)\\b', text))\n","    blob = TextBlob(text)\n","    polarity_score = blob.sentiment.polarity\n","    subjectivity_score = blob.sentiment.subjectivity\n","\n","    # calculate FOG index and syllables per word\n","    fog_index_score = fog_index(word_count, complex_word_count, len(sentences))\n","    syllables_per_word = sum(syllable_count(word) for word in words) / word_count\n","\n","    # calculate percentage of complex words\n","    percentage_of_complex_words = (complex_word_count / word_count) * 100\n","\n","    # create dictionary with analysis results\n","    result = {\n","        \"Word Count\": word_count,\n","        \"Complex Word Count\": complex_word_count,\n","        \"Average Words per Sentence\": avg_words_per_sentence,\n","        \"Average Word Length\": avg_word_length,\n","        \"Personal Pronouns Count\": personal_pronouns,\n","        \"Polarity Score\": polarity_score,\n","        \"Subjectivity Score\": subjectivity_score,\n","        \"FOG Index\": fog_index_score,\n","        \"Syllables per Word\": syllables_per_word,\n","        \"Percentage of complex words\": percentage_of_complex_words,\n","        \"Average Sentence Length\": avg_sentence_length\n","    }\n","\n","    return result\n","\n","\n","# Store results in a list\n","results = []\n","\n","# Loop through all files in the directory\n","for filename in os.listdir('/content/drive/MyDrive/Blackcoffer Project/textFiles1'):\n","    if filename.endswith('.0.txt'):\n","        file_path = os.path.join('/content/drive/MyDrive/Blackcoffer Project/textFiles1', filename)\n","        with open(file_path, 'r', encoding='utf-8') as f:\n","            article = f.read()\n","            # Call both functions and store results in a dictionary\n","            result1 = calculate_variables(filename, article)\n","            result2 = analyze_text(article)\n","            result = {**result1, **result2}\n","            results.append(result)\n","        print(f\"Processed {filename}\")\n","\n","\n","# Write results to output file\n","output_file = \"/content/drive/MyDrive/Blackcoffer Project/Result/output.xlsx\"\n","with open(output_file, 'w') as f:\n","    f.write(\"URL_ID\\tPOSITIVE SCORE\\tNEGATIVE SCORE\\tPOLARITY SCORE\\tSUBJECTIVITY SCORE\")\n","    f.write(\"\\tAVG SENTENCE LENGTH\\tPERCENTAGE OF COMPLEX WORDS\\tFOG INDEX\\tAVG NUMBER OF WORDS PER SENTENCE\\tCOMPLEX WORD COUNT\\tWORD COUNT\\t\")\n","    f.write(\"SYLLABLES PER WORD\\tPERSONAL PRONOUNS\\tAVG WORD LENGTH\\n\")\n","    for result in results:\n","        output_line = f\"{result['file_id'][0:-5]}\\t{result['positive_score']}\\t{result['negative_score']}\\t\"\n","        output_line += f\"{result['polarity_score']:.2f}\\t{result['subjectivity_score']:.2f}\\t{result['Average Sentence Length']}\\t{result['Percentage of complex words']}\\t{result['FOG Index']}\\t\"\n","        output_line += f\"{result['Average Words per Sentence']:.2f}\\t{result['Complex Word Count']:.2f}\\t{result['Word Count']:.2f}\\t{result['Syllables per Word']:.2f}\\t{result['Personal Pronouns Count']:.2f}\\t{result['Average Word Length']:.2f}\\n\"\n","        f.write(output_line)\n","        print(f\"Wrote result for {result['file_id']}\")\n"],"metadata":{"id":"XB_7kqlqVS5y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["output_file = \"/content/drive/MyDrive/Blackcoffer Project/Result/output.xlsx\"\n","with open(output_file, 'w') as f:\n","    f.write(\"URL_ID\\tURL\\tPOSITIVE SCORE\\tNEGATIVE SCORE\\tPOLARITY SCORE\\tSUBJECTIVITY SCORE\")\n","    f.write(\"\\tAVG SENTENCE LENGTH\\tPERCENTAGE OF COMPLEX WORDS\\tFOG INDEX\\tAVG NUMBER OF WORDS PER SENTENCE\\tCOMPLEX WORD COUNT\\tWORD COUNT\\t\")\n","    f.write(\"SYLLABLES PER WORD\\tPERSONAL PRONOUNS\\tAVG WORD LENGTH\\n\")\n","    for i, result in enumerate(results):\n","        output_line = f\"{result['file_id'][0:-5]}\\t{url_list[i]}\\t{result['positive_score']}\\t{result['negative_score']}\\t\"\n","        output_line += f\"{result['polarity_score']:.2f}\\t{result['subjectivity_score']:.2f}\\t{result['Average Sentence Length']:.2f}\\t{result['Percentage of complex words']:.2f}\\t{result['FOG Index']:.2f}\\t\"\n","        output_line += f\"{result['Average Words per Sentence']:.2f}\\t{result['Complex Word Count']:.2f}\\t{result['Word Count']:.2f}\\t{result['Syllables per Word']:.2f}\\t{result['Personal Pronouns Count']:.2f}\\t{result['Average Word Length']:.2f}\\n\"\n","        f.write(output_line)\n","        print(f\"Wrote result for {result['file_id']}\") \n"],"metadata":{"id":"3V_mGBE_-TnR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Conclusion**"],"metadata":{"id":"gCX9965dhzqZ"}},{"cell_type":"markdown","source":["In this project, we extracted textual data articles from the given URLs using Python programming and libraries such as Beautifulsoup and Scrapy. We cleaned the extracted text to remove any irrelevant information and saved each extracted article in a text file with URL_ID as its file name.\n","\n","We then performed textual analysis on each extracted article text using the NLTK library to compute variables such as positive score, negative score, polarity score, subjectivity score, average sentence length, percentage of complex words, fog index, average number of words per sentence, complex word count, word count, syllable per word, personal pronouns, and average word length.\n","\n","We saved the computed variables in a dictionary or a list and populated a pandas dataframe with the extracted variables and the computed variables. We exported the dataframe to a CSV or Excel file as per the format given in the Output Data Structure.xlsx file.\n","\n","In conclusion, we successfully extracted data from the given URLs and performed textual analysis to compute variables as per the given definitions. The project demonstrates the use of Python programming and libraries such as Beautifulsoup, Scrapy, and NLTK for data extraction and analysis."],"metadata":{"id":"Fjb1IsQkh3yE"}}]}